{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HADOOP I_Mío",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CesarPortales/BootCamp/blob/master/HADOOP_I_M%C3%ADo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H__rp74qVhbI",
        "colab_type": "text"
      },
      "source": [
        "# HADOOP desde CERO\n",
        "\n",
        "## Intro\n",
        "\n",
        "No es más que una colección de herramientas open source (algunas hasta closed source).\n",
        "\n",
        "![ML xkcd](https://imgs.xkcd.com/comics/machine_learning.png)\n",
        "\n",
        "## Primero instalemos y probemos HADOOP 2.7.+\n",
        "\n",
        "\n",
        "```\n",
        "docker ps\n",
        "\n",
        "docker run hello-world\n",
        "\n",
        "docker pull sequenceiq/hadoop-docker:2.7.1\n",
        "\n",
        "```\n",
        "### Hasta aqui que ha pasado?\n",
        "\n",
        "[A la pizarra!](https://witeboard.com/9734d460-7306-11e8-904a-07457219cdcb)  \n",
        "\n",
        "[Mejor en G Draw](https://docs.google.com/drawings/d/1-OmMtvMg1STC2RIMlmL4gCxi6MB_3evlwsOhn4RgeVU)\n",
        "\n",
        "http://merrigrove.blogspot.com/2015/10/visualizing-docker-containers-and-images.html\n",
        "\n",
        "### Entremos en accion\n",
        "\n",
        "```\n",
        "docker run -p 50070:50070 -p 8088:8088 -it  sequenceiq/hadoop-docker:2.7.1 /bin/bash -bash\n",
        "\n",
        "/etc/bootstrap.sh\n",
        "\n",
        "ls\n",
        "\n",
        "pwd\n",
        "\n",
        "cat\n",
        "\n",
        "tail\n",
        "\n",
        "mkdir\n",
        "\n",
        "```\n",
        "\n",
        "NOTA: Si encuentras errores de este tipo:    \n",
        "![docker error](https://i.imgur.com/F0lTXv5.jpg)  \n",
        "Tras ejecutar `docker run`, debes intentar reiniciar docker, o cambiar el primer puerto del parametro `-p` porque seguramente exista otro servicio ejecutandose en esos puertos (como apache).  \n",
        "Por ejemplo: `docker run -p 127.0.0.1:50050:50070 -p 127.0.0.1:9988:8088 -it  sequenceiq/hadoop-docker:2.7.1 /bin/bash -bash`\n",
        "\n",
        "\n",
        "### Es HADOOP UNIX sin mas?\n",
        "\n",
        "Hablemos de POSIX:\n",
        "\n",
        "Es un estándar para sistemos operativos.\n",
        "\n",
        "En Unix hay un Kernel, y alrededor tiene un shell, que es el que permite interactuar con el kernel.\n",
        "Es un interfaz que permite interactuar con el sistema de ficheros.\n",
        "\n",
        "El hdfs es una copia de ese shell\n",
        "\n",
        "\n",
        "- **P**ortable **O**perating **S**ystem **I**nterface (UNI**X**)\n",
        "- Creado por Richard Stallman\n",
        "- *PORTABLE*\n",
        "- HDFS no es POSIX\n",
        "\n",
        "![posix](https://fossbytes.com/wp-content/uploads/2016/09/posix-ieee.jpg)\n",
        "\n",
        "\n",
        "https://www.slideshare.net/mcsrivas/design-scale-and-performance-of-maprs-distribution-for-hadoop\n",
        "\n",
        "**RAZON PRINCIPAL**: \"A key one is that you can only add data to the end of the file, not seek to the middle and write things. You cannot seamlessly map code that assumes that all filesystems are Posix-compatible to HDFS.\"\n",
        "  \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9LoLj7YONjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60oLWepON4Jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJy51wTUmjem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plYW9_SwjY-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz2cnu7AjJGr",
        "colab_type": "text"
      },
      "source": [
        "## HDFS\n",
        "\n",
        "![data store](http://csis.pace.edu/~marchese/CS865/Lectures/Chap7/Chapter7fin_files/image002.jpg)\n",
        "\n",
        "### Que debe existir dentro de un HDFS?\n",
        "\n",
        "- Ficheros inmutables (csv, txt, etc)\n",
        "- Jars de mapreduce\n",
        "- Librerias\n",
        "- Ficheros de secuencia\n",
        "\n",
        "Tendremos un data store distribuído en varios nodos\n",
        "\n",
        "### Juguemos con HDFS\n",
        "\n",
        "```\n",
        "cd $HADOOP_PREFIX\n",
        "\n",
        "bin/hdfs dfs -mkdir pruebas\n",
        "\n",
        "bin/hdfs dfs -put README.txt pruebas/README.txt # Va a fallar si no existe la carpeta en hdfs\n",
        "\n",
        "bin/hdfs dfs -ls pruebas\n",
        "\n",
        "bin/hdfs dfs -touchz pruebas/test.zip\n",
        "\n",
        "bin/hdfs dfs -ls pruebas\n",
        "\n",
        "bin/hdfs dfs -cat pruebas/*\n",
        "\n",
        "bin/hdfs dfs\n",
        "\n",
        "jps  --> me dice qué servicios se están ejecutando en esta máquina. Viene de java virtual machine\n",
        "\n",
        "permite hacer otro niveol de abstracción sobre los recursos. Permite a Java controlar los recursos sin pegarse con el SO.\n",
        "ps --> todos los procesos que tengo corriendo (viene de powershell)\n",
        "bin/hdfs dfs -du -h  --> información de mi unidad (sin el -h el espacio total, con el -h el espacio ocupado)\n",
        "```\n",
        "\n",
        "\n",
        "## HIGH AVAILABILITY\n",
        "- FAULT TOLERANCE\n",
        "- Resilient\n",
        "- Elasticity\n",
        "\n",
        "\n",
        " ### JPS???\n",
        " \n",
        " Toda la arquitectura de hadoop está montada sobre java\n",
        " \n",
        " JPS es java processes\n",
        " \n",
        " ![jps arch](https://hadoop.apache.org/docs/r1.2.1/images/hdfsarchitecture.gif)\n",
        "\n",
        "* HDFS tiene una arquitectura maestro/esclavo. \n",
        "* Un clúster HDFS consta de un único NameNode, un servidor maestro que administra el espacio de nombres del sistema de archivos y regula el acceso a los archivos por parte de los clientes. \n",
        "* Los DataNodes administran el almacenamiento.\n",
        "\n",
        "### Arquitectura HDFS\n",
        "\n",
        "\n",
        "Un datanode es un servicio que solo me da acceso a datos. Es una pieza de soft que me deja guardar o leer o borrar, ... sobre datos.\n",
        "\n",
        "Los datanodes son como cajas en las que van los ficheros (o parte de los ficheros). Estas cajas no tienen inteligencia ninguan. Los gestiona un namenode, que es una máquina que sabe dónde están todos los datanodes.\n",
        "El namenode tiene la ip, puerto, usuario, etc de cada datanode. Y no solo eso: también sabe dónde están las réplicas.\n",
        "\n",
        "Cuando se ejecutan los comandos que hemos visto, se pueden ejecutar sobre el namenode o sobre el datanode. Si lo hago sobrer este, lo ejecuta sobre todos los datanodes. (es parecido al maestro y el esclavo).\n",
        "\n",
        "Secundary namenode --> el hadoop obliga a levantarlo por si falla el datanode. Si este se cae, hadoop automáticamente levanta el secondary.\n",
        "\n",
        "el namenode puede ser una máquina dedicada o no, y no necesita gran capacidad porque no almacena datos (los datos se guardan en los datanodes).\n",
        "\n",
        "Cuando se trabaja en la nube, hablamos de máquinas gigantescas, con un software de virtualización que me hace creer que una parte de esa máquina es un ordenador (lo puedo hacer con docker o con lo que sea).\n",
        "\n",
        "Los datanodes se pueden agrupar dentro de un rack. Ek rack es físico.\n",
        "el client del esquema es, en nuestro caso la consola. Es un elemento de interacción con la arquitectura.\n",
        "\n",
        "Hadoop lo inventó yahoo (cree). \n",
        "\n",
        "Hasta la ubicación física de los nodos es importantísima.\n",
        "\n",
        "* Los NameNode y DataNode son piezas de software diseñadas para ejecutarse en máquinas básicas. \n",
        "* Estas máquinas suelen ejecutar un sistema operativo GNU / Linux. \n",
        "* HDFS está construido usando en Java.\n",
        "* La existencia de un NameNode único en un clúster simplifica enormemente la arquitectura del sistema. \n",
        "* NameNode es el árbitro y repositorio de todos los metadatos HDFS. \n",
        "* El sistema está diseñado de tal manera que los datos del usuario nunca fluyan a través del NameNode.\n",
        "* La ubicacion fisica de los nodos es de vital importancia y requiere mucha practica (ensayo y error).\n",
        "\n",
        "### Mas lectura:\n",
        "\n",
        "- https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\n",
        "- https://fossbytes.com/posix-what-is-the-portable-operating-system-interface/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypNs-39hxtV1",
        "colab_type": "text"
      },
      "source": [
        "No estamos aún en Hadoop. \n",
        "\n",
        "El archivo bootstrap arranca el dfs y arranca el yarm. HDfs no es un sistema operativo. son varios ficheros ejecutables que lanzo y olvido\n",
        "\n",
        "el HDFS  (Hadoop distributed file system)--> Han cogido el sistmea jerárquico de carpetas y archivos y lo han distribuído en varías máquinas.\n",
        "\n",
        "Es una de las grandes herramientas que forman parte de Hadoop.\n",
        "\n",
        "el hdfs me permite hacer todo esto con un cluster de Máquinas --> puedo tener 1, 2 20 máquinas que me soporten el sistema de ficheros.\n",
        "\n",
        "HDFS distribuye la información por todos los nodos y hace un Data-store distribuído, y puedo tener 20 procesos leyendo esos datos en los 20 ficheros.\n",
        "\n",
        "Para acceder al sistema de hadoop\n",
        "\n",
        "**cd /usr/local/hadoop**\n",
        "\n",
        "Si hago un ls, veo la carpeta bin, que tiene todos los comandos que puedo ejecutar en el HDFS.\n",
        "\n",
        "Si quiero crear una carpeta  llamada pruebas\n",
        "\n",
        "bin/hdfs dfs -mkdir pruebas\n",
        "\n",
        "Esta carpeta la habrá creado dentro de /user/root. Para verla, no basta un simple ls\n",
        "\n",
        "**bin/hdfs dfs -ls /user/root**\n",
        "Así que estamos trabajando con dos niveles. Está por un lado el SO del container (un Unix) , y luego hay un nivel por encima que es el del sistema de ficheros distribuído.\n",
        "este sistema solo está accesible a través de esta herramienta que es el DFS.\n",
        "\n",
        "si quiero copiar un archivo, tengo que hacer un -put\n",
        "\n",
        "bin/hdfs dfs -put README.txt pruebas/README.txt #el fichero README.txt exite en la carpeta  /usr/local/hadoop\n",
        "\n",
        "Para ver que me lo ha copiado\n",
        "\n",
        "bin/hdfs dfs -ls /user/root/pruebas\n",
        "\n",
        "\n",
        "si quiero ver el contenido del archivo\n",
        "\n",
        "**hdfs dfs -cat pruebas/README.txt**\n",
        "\n",
        "\n",
        "Si quiero ver todo lo que se puede hacer:\n",
        "\n",
        " **bin/hdfs dfs**\n",
        " \n",
        " \n",
        " http://127.0.0.1:50070/dfshealth.html#tab-overview\n",
        " \n",
        "el puerto 50070 es el puerto de HDFS.\n",
        "\n",
        "Conesto accedo a toda la información de mi HDFS (Gigas libres, gigas ocupados,etc.)\n",
        "\n",
        "si tuvier un cluster, en la pestaña Datanodes vería información \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ0zHFKREBWt",
        "colab_type": "text"
      },
      "source": [
        "![texto alternativo](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BdYzPp9wz-z",
        "colab_type": "text"
      },
      "source": [
        "el HDFS --> Han cogido el sistmea jerárquico de carpetas y archivos y lo han distribuído en varías máquinas.\n",
        "Es una de las grandes herramientas que forman parte de Hadoop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbTapS2Tb0J4",
        "colab_type": "text"
      },
      "source": [
        "hasta ahora solo hemos visgto como meter ficheros en hdfs.\n",
        "siempre que se instala haddop se introducen los examples. Es un java compilado con un montón de ejemplos de cosas que se pueden hacer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltlVqo1MjJh3",
        "colab_type": "text"
      },
      "source": [
        "## MAP REDUCE\n",
        "\n",
        "### Juguemos con Map Reduce\n",
        "\n",
        "Le vamos a pedir que se ejecute el jar de hadoop.  El comando grep busca texto dentro de una serie de ficheros. Nos vamos a meter dentro de la carpeta input y vamos abuscar un texto y el resultaod queremos que lo busque en la carpeta outpu\n",
        "\n",
        "```\n",
        "cd $HADOOP_PREFIX\n",
        "\n",
        "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output 'dfs[a-z.]+'\n",
        "\n",
        "```\n",
        "El comando primero hace un mapeo, y debería haber escrito algo en la carpeta output.\n",
        "Si ejecutamos:\n",
        "bin/hdfs dfs -cat output/*\n",
        "podremos ver qué ha creado\n",
        "### Pero que ha pasado?\n",
        "\n",
        "docker crea una red virtual. cuando estaba haciendo el mapeo, estaba cogiendo todas las líneas de los ficheros , busca todas las palabras y crea pares clave/ valor.\n",
        "Luego hace el shuffing y lo reduce\n",
        "\n",
        "Busquemos la IP del cluster, desde nuestro host:\n",
        "\n",
        "```\n",
        "docker network list\n",
        "\n",
        "docker network inspect docker_default\n",
        "\n",
        "```\n",
        "Fijarse en gateway, y cambiar el `1` por un `2`.\n",
        "\n",
        "Visitemos: http://127.0.0.1:50070 * NOTA - WINDOWS - En docker ANTES de hacer el run aparece el IP en verde, de esta manera:\n",
        "![winip](https://i.imgur.com/Z8g9n4d.png)\n",
        "\n",
        "\n",
        "### Que otra forma tendriamos de ver los resultados?\n",
        "\n",
        "\n",
        "### Un poco de Teoria obligatoria\n",
        "\n",
        "map: reading and formatting data\n",
        "reduce: applying transformations/operations on all of the data\n",
        "\n",
        "![mr](https://www.todaysoftmag.com/images/articles/tsm33/large/a11.png)\n",
        "\n",
        "\n",
        "### Veamos el Codigo --> siemrpe tendrá dos partes, el map y el reduce\n",
        "\n",
        "```java\n",
        "import java.io.IOException;\n",
        "import java.util.StringTokenizer;\n",
        "\n",
        "import org.apache.hadoop.conf.Configuration;\n",
        "import org.apache.hadoop.fs.Path;\n",
        "import org.apache.hadoop.io.IntWritable;\n",
        "import org.apache.hadoop.io.Text;\n",
        "import org.apache.hadoop.mapreduce.Job;\n",
        "import org.apache.hadoop.mapreduce.Mapper;\n",
        "import org.apache.hadoop.mapreduce.Reducer;\n",
        "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
        "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
        "\n",
        "public class WordCount {\n",
        "\n",
        "  public static class TokenizerMapper\n",
        "       extends Mapper<Object, Text, Text, IntWritable>{\n",
        "\n",
        "    private final static IntWritable one = new IntWritable(1);\n",
        "    private Text word = new Text();\n",
        "\n",
        "    public void map(Object key, Text value, Context context\n",
        "                    ) throws IOException, InterruptedException {\n",
        "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
        "      while (itr.hasMoreTokens()) {\n",
        "        word.set(itr.nextToken());\n",
        "        context.write(word, one);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  public static class IntSumReducer\n",
        "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
        "    private IntWritable result = new IntWritable();\n",
        "\n",
        "    public void reduce(Text key, Iterable<IntWritable> values,\n",
        "                       Context context\n",
        "                       ) throws IOException, InterruptedException {\n",
        "      int sum = 0;\n",
        "      for (IntWritable val : values) {\n",
        "        sum += val.get();\n",
        "      }\n",
        "      result.set(sum);\n",
        "      context.write(key, result);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  public static void main(String[] args) throws Exception {\n",
        "    Configuration conf = new Configuration();\n",
        "    Job job = Job.getInstance(conf, \"word count\");\n",
        "    job.setJarByClass(WordCount.class);\n",
        "    job.setMapperClass(TokenizerMapper.class);\n",
        "    job.setCombinerClass(IntSumReducer.class);\n",
        "    job.setReducerClass(IntSumReducer.class);\n",
        "    job.setOutputKeyClass(Text.class);\n",
        "    job.setOutputValueClass(IntWritable.class);\n",
        "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
        "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
        "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### Mas ejemplos\n",
        "\n",
        "```\n",
        "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar\n",
        "```\n",
        "\n",
        "### Mas lectura\n",
        "\n",
        "- https://www.todaysoftmag.com/article/1358/hadoop-mapreduce-deep-diving-and-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xgqpiTrpP-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4MUlSSgLSFR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykOw12sMLSXt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}