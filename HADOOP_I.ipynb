{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HADOOP I",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CesarPortales/BootCamp/blob/master/HADOOP_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H__rp74qVhbI",
        "colab_type": "text"
      },
      "source": [
        "# HADOOP desde CERO\n",
        "\n",
        "## Intro\n",
        "\n",
        "No es más que una colección de herramientas open source (algunas hasta closed source).\n",
        "\n",
        "![ML xkcd](https://imgs.xkcd.com/comics/machine_learning.png)\n",
        "\n",
        "## Primero instalemos y probemos HADOOP 2.7.+\n",
        "\n",
        "\n",
        "```\n",
        "docker ps\n",
        "\n",
        "docker run hello-world\n",
        "\n",
        "docker pull sequenceiq/hadoop-docker:2.7.1\n",
        "\n",
        "```\n",
        "### Hasta aqui que ha pasado?\n",
        "\n",
        "[A la pizarra!](https://witeboard.com/9734d460-7306-11e8-904a-07457219cdcb)  \n",
        "\n",
        "[Mejor en G Draw](https://docs.google.com/drawings/d/1-OmMtvMg1STC2RIMlmL4gCxi6MB_3evlwsOhn4RgeVU)\n",
        "\n",
        "http://merrigrove.blogspot.com/2015/10/visualizing-docker-containers-and-images.html\n",
        "\n",
        "### Entremos en accion\n",
        "\n",
        "```\n",
        "docker run -p 50070:50070 -p 8088:8088 -it  sequenceiq/hadoop-docker:2.7.1 /bin/bash -bash\n",
        "\n",
        "/etc/bootstrap.sh\n",
        "\n",
        "ls\n",
        "\n",
        "pwd\n",
        "\n",
        "cat\n",
        "\n",
        "tail\n",
        "\n",
        "mkdir\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### Es HADOOP UNIX sin mas?\n",
        "\n",
        "Hablemos de POSIX:\n",
        "\n",
        "- **P**ortable **O**perating **S**ystem **I**nterface (UNI**X**)\n",
        "- Creado por Richard Stallman\n",
        "- *PORTABLE*\n",
        "- HDFS no es POSIX\n",
        "\n",
        "![posix](https://fossbytes.com/wp-content/uploads/2016/09/posix-ieee.jpg)\n",
        "\n",
        "\n",
        "https://www.slideshare.net/mcsrivas/design-scale-and-performance-of-maprs-distribution-for-hadoop\n",
        "\n",
        "**RAZON PRINCIPAL**: \"A key one is that you can only add data to the end of the file, not seek to the middle and write things. You cannot seamlessly map code that assumes that all filesystems are Posix-compatible to HDFS.\"\n",
        "  \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9LoLj7YONjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60oLWepON4Jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJy51wTUmjem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plYW9_SwjY-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz2cnu7AjJGr",
        "colab_type": "text"
      },
      "source": [
        "## HDFS\n",
        "\n",
        "![data store](http://csis.pace.edu/~marchese/CS865/Lectures/Chap7/Chapter7fin_files/image002.jpg)\n",
        "\n",
        "### Que debe existir dentro de un HDFS?\n",
        "\n",
        "- Ficheros inmutables (csv, txt, etc)\n",
        "- Jars de mapreduce\n",
        "- Librerias\n",
        "- Ficheros de secuencia\n",
        "\n",
        "### Juguemos con HDFS\n",
        "\n",
        "```\n",
        "cd $HADOOP_PREFIX\n",
        "\n",
        "bin/hdfs dfs -mkdir pruebas\n",
        "\n",
        "bin/hdfs dfs -put README.txt pruebas/README.txt # Va a fallar si no existe la carpeta en hdfs\n",
        "\n",
        "bin/hdfs dfs -ls pruebas\n",
        "\n",
        "bin/hdfs dfs -touchz pruebas/test.zip\n",
        "\n",
        "bin/hdfs dfs -ls pruebas\n",
        "\n",
        "bin/hdfs dfs -cat pruebas/*\n",
        "\n",
        "bin/hdfs dfs\n",
        "\n",
        "jps\n",
        "```\n",
        "\n",
        "\n",
        "## HIGH AVAILABILITY\n",
        "- FAULT TOLERANCE\n",
        "- Resilient\n",
        "- Elasticity\n",
        "\n",
        "\n",
        " ### JPS???\n",
        " \n",
        " ![jps arch](https://hadoop.apache.org/docs/r1.2.1/images/hdfsarchitecture.gif)\n",
        "\n",
        "* HDFS tiene una arquitectura maestro/esclavo. \n",
        "* Un clúster HDFS consta de un único NameNode, un servidor maestro que administra el espacio de nombres del sistema de archivos y regula el acceso a los archivos por parte de los clientes. \n",
        "* Los DataNodes administran el almacenamiento.\n",
        "\n",
        "### Arquitectura HDFS\n",
        "\n",
        "* Los NameNode y DataNode son piezas de software diseñadas para ejecutarse en máquinas básicas. \n",
        "* Estas máquinas suelen ejecutar un sistema operativo GNU / Linux. \n",
        "* HDFS está construido usando en Java.\n",
        "* La existencia de un NameNode único en un clúster simplifica enormemente la arquitectura del sistema. \n",
        "* NameNode es el árbitro y repositorio de todos los metadatos HDFS. \n",
        "* El sistema está diseñado de tal manera que los datos del usuario nunca fluyan a través del NameNode.\n",
        "* La ubicacion fisica de los nodos es de vital importancia y requiere mucha practica (ensayo y error).\n",
        "\n",
        "### Mas lectura:\n",
        "\n",
        "- https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\n",
        "- https://fossbytes.com/posix-what-is-the-portable-operating-system-interface/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHr6zpmN4pGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltlVqo1MjJh3",
        "colab_type": "text"
      },
      "source": [
        "## MAP REDUCE\n",
        "\n",
        "### Juguemos con Map Reduce\n",
        "\n",
        "```\n",
        "cd $HADOOP_PREFIX\n",
        "\n",
        "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output 'dfs[a-z.]+'\n",
        "\n",
        "```\n",
        "\n",
        "### Pero que ha pasado?\n",
        "\n",
        "Busquemos la IP del cluster, desde nuestro host:\n",
        "\n",
        "```\n",
        "docker network list\n",
        "\n",
        "docker network inspect docker_default\n",
        "\n",
        "```\n",
        "Fijarse en gateway, y cambiar el `1` por un `2`.\n",
        "\n",
        "Visitemos: http://127.0.0.1:50070 * NOTA - WINDOWS - En docker ANTES de hacer el run aparece el IP en verde, de esta manera:\n",
        "![winip](https://i.imgur.com/Z8g9n4d.png)\n",
        "\n",
        "\n",
        "### Que otra forma tendriamos de ver los resultados?\n",
        "\n",
        "\n",
        "### Un poco de Teoria obligatoria\n",
        "\n",
        "map: reading and formatting data\n",
        "reduce: applying transformations/operations on all of the data\n",
        "\n",
        "![mr](https://www.todaysoftmag.com/images/articles/tsm33/large/a11.png)\n",
        "\n",
        "\n",
        "### Veamos el Codigo\n",
        "\n",
        "```java\n",
        "import java.io.IOException;\n",
        "import java.util.StringTokenizer;\n",
        "\n",
        "import org.apache.hadoop.conf.Configuration;\n",
        "import org.apache.hadoop.fs.Path;\n",
        "import org.apache.hadoop.io.IntWritable;\n",
        "import org.apache.hadoop.io.Text;\n",
        "import org.apache.hadoop.mapreduce.Job;\n",
        "import org.apache.hadoop.mapreduce.Mapper;\n",
        "import org.apache.hadoop.mapreduce.Reducer;\n",
        "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
        "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
        "\n",
        "public class WordCount {\n",
        "\n",
        "  public static class TokenizerMapper\n",
        "       extends Mapper<Object, Text, Text, IntWritable>{\n",
        "\n",
        "    private final static IntWritable one = new IntWritable(1);\n",
        "    private Text word = new Text();\n",
        "\n",
        "    public void map(Object key, Text value, Context context\n",
        "                    ) throws IOException, InterruptedException {\n",
        "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
        "      while (itr.hasMoreTokens()) {\n",
        "        word.set(itr.nextToken());\n",
        "        context.write(word, one);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  public static class IntSumReducer\n",
        "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
        "    private IntWritable result = new IntWritable();\n",
        "\n",
        "    public void reduce(Text key, Iterable<IntWritable> values,\n",
        "                       Context context\n",
        "                       ) throws IOException, InterruptedException {\n",
        "      int sum = 0;\n",
        "      for (IntWritable val : values) {\n",
        "        sum += val.get();\n",
        "      }\n",
        "      result.set(sum);\n",
        "      context.write(key, result);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  public static void main(String[] args) throws Exception {\n",
        "    Configuration conf = new Configuration();\n",
        "    Job job = Job.getInstance(conf, \"word count\");\n",
        "    job.setJarByClass(WordCount.class);\n",
        "    job.setMapperClass(TokenizerMapper.class);\n",
        "    job.setCombinerClass(IntSumReducer.class);\n",
        "    job.setReducerClass(IntSumReducer.class);\n",
        "    job.setOutputKeyClass(Text.class);\n",
        "    job.setOutputValueClass(IntWritable.class);\n",
        "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
        "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
        "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### Mas ejemplos\n",
        "\n",
        "```\n",
        "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar\n",
        "```\n",
        "\n",
        "### Mas lectura\n",
        "\n",
        "- https://www.todaysoftmag.com/article/1358/hadoop-mapreduce-deep-diving-and-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xgqpiTrpP-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}